# EvoSpec CLI â€” Environment Variables
# Copy this file to .env and fill in your API keys

# ============================================
# LLM Provider API Keys
# ============================================
# You only need to set the key for the provider you want to use.
# Default provider is OpenRouter.

# OpenRouter (Default, Recommended)
# Get your API key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# OpenAI
# Get your API key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here

# Anthropic
# Get your API key at: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=sk-ant-your-key-here

# ============================================
# Model Selection (Optional)
# ============================================
# Override the default model for each provider.
# If not set, defaults from config are used.

# OpenRouter models (see https://openrouter.ai/models)
# Default: anthropic/claude-sonnet-4.5
OPENROUTER_MODEL=anthropic/claude-sonnet-4.5
# Other options:
#   anthropic/claude-opus-4.1 (most capable)
#   openai/gpt-5.2
#   openai/gpt-5.2-pro
#   google/gemini-2.0-pro
#   meta-llama/llama-4-maverick

# OpenAI models
# Default: gpt-5.2
# OPENAI_MODEL=gpt-5.2

# Anthropic models (direct API)
# Default: claude-sonnet-4-5
# ANTHROPIC_MODEL=claude-sonnet-4-5

# Ollama models (local)
# Default: llama4
# OLLAMA_MODEL=llama4

# ============================================
# Ollama (Local)
# ============================================
# No API key required for Ollama.
# Make sure Ollama is running locally: https://ollama.ai
# Default URL: http://localhost:11434

# ============================================
# Notes
# ============================================
# - Only one provider API key is required
# - OpenRouter is recommended as it provides access to multiple models
# - For local/private use, Ollama requires no API key
# - Never commit your actual API keys to version control
